{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumitrapy/da6401_assignment2/blob/main/partB/notebook2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ebc1a698",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebc1a698",
        "outputId": "0363290a-5efb-476b-bbe1-a072a67036b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 33 (delta 8), reused 28 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (33/33), 16.90 KiB | 692.00 KiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "/content/project\n",
            "--2025-04-22 04:52:19--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.12.207, 172.217.194.207, 142.250.4.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.12.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3816687935 (3.6G) [application/zip]\n",
            "Saving to: ‘nature_12K.zip’\n",
            "\n",
            "nature_12K.zip      100%[===================>]   3.55G  20.8MB/s    in 2m 52s  \n",
            "\n",
            "2025-04-22 04:55:11 (21.2 MB/s) - ‘nature_12K.zip’ saved [3816687935/3816687935]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/soumitrapy/da6401_assignment2.git project\n",
        "# %cd project\n",
        "# !wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
        "# !unzip -q nature_12K.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3c96c29f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c96c29f",
        "outputId": "d99630ec-342a-4441-9747-451dfc659f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project/partB\n"
          ]
        }
      ],
      "source": [
        "# !git pull\n",
        "# %cd partB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9d3e977e",
      "metadata": {
        "id": "9d3e977e"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import yaml\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader, random_split\n",
        "\n",
        "from torchvision import transforms\n",
        "#from torchvision.io import read_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1b8b8420",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b8b8420",
        "outputId": "1a82ea30-b666-4ca4-d9de-aa8a02b8591b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project': 'dla2partb',\n",
              " 'use_wandb': True,\n",
              " 'dataset': {'name': 'CustomDataset',\n",
              "  'path': '../data/inaturalist_12K/',\n",
              "  'img_size': 224,\n",
              "  'class_names': ['Plantae',\n",
              "   'Mammalia',\n",
              "   'Animalia',\n",
              "   'Reptilia',\n",
              "   'Amphibia',\n",
              "   'Aves',\n",
              "   'Fungi',\n",
              "   'Arachnida',\n",
              "   'Mollusca',\n",
              "   'Insecta'],\n",
              "  'batch_size': 100},\n",
              " 'model': {'name': 'resnet50', 'in_channels': 3, 'num_classes': 10},\n",
              " 'train': {'epochs': 20, 'val_interval': 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import yaml\n",
        "config = yaml.safe_load(open(\"config/pretrained.yaml\"))\n",
        "\n",
        "# config['dataset']['batch_size']=32\n",
        "config['train']['epochs']=20\n",
        "config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "499ece3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "499ece3a",
        "outputId": "75287d0a-75e5-42d4-d807-95c0dc238b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoumitrapy\u001b[0m (\u001b[33msoumitrapy-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "if config.get('use_wandb',False):\n",
        "    wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb0e086",
      "metadata": {
        "id": "6cb0e086"
      },
      "source": [
        "### DataLoader Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1245ca4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1245ca4e",
        "outputId": "7b9e88ac-3fd9-4692-be45-0f558c21b5ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train len: 9999, val len: 2000\n",
            "device = cuda\n"
          ]
        }
      ],
      "source": [
        "from preprocessing import CustomDataset\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "cfg = config['dataset']\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(cfg['img_size']),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(cfg['img_size']),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "splits = ['train', 'val']\n",
        "datasets = {x:CustomDataset(path= cfg['path']+x,\n",
        "                            class_names=cfg['class_names'],\n",
        "                            transform=data_transforms[x])\n",
        "            for x in splits}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x],\n",
        "                                              batch_size=cfg['batch_size'],\n",
        "                                              shuffle=True\n",
        "                                              )\n",
        "                for x in splits}\n",
        "\n",
        "class_names = datasets['train'].class_names\n",
        "print(f\"train len: {len(datasets['train'])}, val len: {len(datasets['val'])}\")\n",
        "\n",
        "# We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n",
        "# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device = {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2213598f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2213598f",
        "outputId": "b2e9f5d2-d6d4-4c6f-adc6-d40e5b5b939c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 3, 224, 224]) torch.Size([100])\n"
          ]
        }
      ],
      "source": [
        "for x, y in dataloaders['train']:\n",
        "    print(x.shape,y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c87e80a",
      "metadata": {
        "id": "8c87e80a"
      },
      "source": [
        "### Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "79c89927",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79c89927",
        "outputId": "58a3dfbb-a589-43d8-a045-2643515fd0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 65.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from models import get_model\n",
        "model = get_model(config['model'])\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f4124940",
      "metadata": {
        "id": "f4124940"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b223be",
      "metadata": {
        "id": "92b223be"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "27827cb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "27827cb3",
        "outputId": "6cbe1d27-95a2-40f8-f2aa-dce5af5c6477"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/project/partB/wandb/run-20250422_045939-raowoepa</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/soumitrapy-iit-madras/dla2partb/runs/raowoepa' target=\"_blank\">ResNet22 04:59:3</a></strong> to <a href='https://wandb.ai/soumitrapy-iit-madras/dla2partb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/soumitrapy-iit-madras/dla2partb' target=\"_blank\">https://wandb.ai/soumitrapy-iit-madras/dla2partb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/soumitrapy-iit-madras/dla2partb/runs/raowoepa' target=\"_blank\">https://wandb.ai/soumitrapy-iit-madras/dla2partb/runs/raowoepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 100/100 [02:35<00:00,  1.55s/it, train_accuracy=59.7, train_loss=1.42]\n",
            "Validation: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, val_accuracy=72.6, val_loss=1.01]\n",
            "Epoch 2: 100%|██████████| 100/100 [02:22<00:00,  1.43s/it, train_accuracy=74.2, train_loss=0.914]\n",
            "Validation: 100%|██████████| 20/20 [00:29<00:00,  1.49s/it, val_accuracy=76.3, val_loss=0.833]\n",
            "Epoch 3: 100%|██████████| 100/100 [02:15<00:00,  1.35s/it, train_accuracy=77.2, train_loss=0.787]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.27s/it, val_accuracy=79.5, val_loss=0.722]\n",
            "Epoch 4: 100%|██████████| 100/100 [02:13<00:00,  1.34s/it, train_accuracy=77.7, train_loss=0.735]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.30s/it, val_accuracy=78.2, val_loss=0.729]\n",
            "Epoch 5: 100%|██████████| 100/100 [02:12<00:00,  1.33s/it, train_accuracy=78.8, train_loss=0.693]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it, val_accuracy=78.9, val_loss=0.679]\n",
            "Epoch 6: 100%|██████████| 100/100 [02:14<00:00,  1.34s/it, train_accuracy=80.2, train_loss=0.651]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it, val_accuracy=79.9, val_loss=0.637]\n",
            "Epoch 7: 100%|██████████| 100/100 [02:12<00:00,  1.33s/it, train_accuracy=79.8, train_loss=0.64]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.28s/it, val_accuracy=79.7, val_loss=0.648]\n",
            "Epoch 8: 100%|██████████| 100/100 [02:13<00:00,  1.33s/it, train_accuracy=80.6, train_loss=0.617]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it, val_accuracy=80, val_loss=0.631]\n",
            "Epoch 9: 100%|██████████| 100/100 [02:14<00:00,  1.34s/it, train_accuracy=81.4, train_loss=0.601]\n",
            "Validation: 100%|██████████| 20/20 [00:26<00:00,  1.30s/it, val_accuracy=79.7, val_loss=0.637]\n",
            "Epoch 10: 100%|██████████| 100/100 [02:12<00:00,  1.32s/it, train_accuracy=81.6, train_loss=0.584]\n",
            "Validation: 100%|██████████| 20/20 [00:26<00:00,  1.31s/it, val_accuracy=80.8, val_loss=0.609]\n",
            "Epoch 11: 100%|██████████| 100/100 [02:14<00:00,  1.35s/it, train_accuracy=81.3, train_loss=0.578]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it, val_accuracy=80.6, val_loss=0.625]\n",
            "Epoch 12: 100%|██████████| 100/100 [02:11<00:00,  1.31s/it, train_accuracy=82.3, train_loss=0.565]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.27s/it, val_accuracy=80.7, val_loss=0.597]\n",
            "Epoch 13: 100%|██████████| 100/100 [02:13<00:00,  1.34s/it, train_accuracy=82.4, train_loss=0.57]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.27s/it, val_accuracy=79.8, val_loss=0.637]\n",
            "Epoch 14: 100%|██████████| 100/100 [02:12<00:00,  1.32s/it, train_accuracy=82.3, train_loss=0.568]\n",
            "Validation: 100%|██████████| 20/20 [00:26<00:00,  1.34s/it, val_accuracy=79.2, val_loss=0.635]\n",
            "Epoch 15: 100%|██████████| 100/100 [02:13<00:00,  1.33s/it, train_accuracy=82.2, train_loss=0.57]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it, val_accuracy=80.4, val_loss=0.637]\n",
            "Epoch 16: 100%|██████████| 100/100 [02:12<00:00,  1.32s/it, train_accuracy=82.2, train_loss=0.57]\n",
            "Validation: 100%|██████████| 20/20 [00:26<00:00,  1.31s/it, val_accuracy=79.7, val_loss=0.61]\n",
            "Epoch 17: 100%|██████████| 100/100 [02:13<00:00,  1.33s/it, train_accuracy=82.9, train_loss=0.554]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.28s/it, val_accuracy=79.8, val_loss=0.631]\n",
            "Epoch 18: 100%|██████████| 100/100 [02:13<00:00,  1.33s/it, train_accuracy=81.9, train_loss=0.573]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.28s/it, val_accuracy=80.2, val_loss=0.625]\n",
            "Epoch 19: 100%|██████████| 100/100 [02:12<00:00,  1.32s/it, train_accuracy=82.4, train_loss=0.561]\n",
            "Validation: 100%|██████████| 20/20 [00:26<00:00,  1.31s/it, val_accuracy=81.5, val_loss=0.588]\n",
            "Epoch 20: 100%|██████████| 100/100 [02:11<00:00,  1.32s/it, train_accuracy=82.1, train_loss=0.565]\n",
            "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.28s/it, val_accuracy=80.5, val_loss=0.61]\n"
          ]
        }
      ],
      "source": [
        "from train import train\n",
        "ru = None\n",
        "if config['use_wandb']:\n",
        "    run_name = type(model).__name__+str(datetime.now())[8:18]\n",
        "    wandb.init(\n",
        "        project=config['project'],\n",
        "        name = run_name,\n",
        "        config=config['model'],\n",
        "    )\n",
        "train(model=model, optimizer=optimizer, loss_fn=loss_fn, dataloaders=dataloaders,config=config['train'], model_config=config['model'], scheduler = scheduler, device = device, use_wandb = config['use_wandb'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c16df84",
      "metadata": {
        "id": "7c16df84"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=config['project'],\n",
        "    name = type(model).__name__+str(datetime.now())[8:18],\n",
        "    config=config['model'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e375f9e",
      "metadata": {
        "id": "4e375f9e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "813fe4c1",
      "metadata": {
        "id": "813fe4c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_artifact = wandb.use_artifact(\"trained-model:latest\")\n",
        "model_dir = model_artifact.download()\n",
        "model_path = os.path.join(model_dir, \"trained_model.pt\")\n",
        "model_config = model_artifact.metadata\n",
        "\n",
        "model = DefaultModel(**model_config)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "\n",
        "    loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)\n",
        "\n",
        "    run.summary.update({\"loss\": loss, \"accuracy\": accuracy})\n",
        "\n",
        "    wandb.log({\"high-loss-examples\":\n",
        "        [wandb.Image(hard_example, caption=str(int(pred)) + \",\" +  str(int(label)))\n",
        "            for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50a8214f",
      "metadata": {
        "id": "50a8214f"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "877f5657",
      "metadata": {
        "id": "877f5657"
      },
      "outputs": [],
      "source": [
        "# model_path = './checkpoints/ResNet_cuda2025-04-19 17_1.pth'\n",
        "# model.load_state_dict(torch.load(model_path, map_location=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9696a4db",
      "metadata": {
        "id": "9696a4db"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55431b31",
      "metadata": {
        "id": "55431b31"
      },
      "outputs": [],
      "source": [
        "from train import visualize_model\n",
        "visualize_model(model,valdl, class_names, 100, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d0e141",
      "metadata": {
        "id": "58d0e141"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e774b3",
      "metadata": {
        "id": "f6e774b3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}